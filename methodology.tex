\section{Discussion \& Methodology} 
In order to design and construct a language learning application which is oriented towards practical speaking skills, we must evaluate competing methodologies for L2 acquisition and pragmatically choose the most well-suited attributes of each. Trivially, we have chosen to favor a CALL/CAPT application over the design of a new classroom-based curriculum or other traditional approach. Computers in general are uniquely suited for language training because they have infinite amounts of patience and well-established memory-enhancing methods such as spaced repetition \cite{glanzer1971repetition} \cite{wozniak2007supermemo} \cite{wozniak1990supermemo} \cite{cuddy1982forgetting} are relatively easy to implement programatically.

Unfortunately, computer systems still lack the eponymous ability of trained human langauge instructors to make pointed assessments of speech intelligibility. Machine-based methods in this domain still critically lag human performance and as such we argue humans cannot be cut out of the loop yet. Part of this is undoubtedly due to a lack of large annotated pronunciation assessment corpora for supervised learning. Automatic speech recognition (ASR) technology has been around for decades, and yet only recently with the infusion of massive amounts of annotated data and proportionally massive supercomputing systems has ASR performance in languages like English and Mandarin begun to equal or outstrip human performance \cite{hannun2014deep} \cite{hinton2012deep} \cite{chan2016listen}. 

Pronunciation and intelligibility assessment can be mechanistically described as process of assessing an input speech signal and generating human language to describe how to improve or change the signal to better match some target speech signal(s). To offer a practical example from a Pimsleur-style language guide for Akan:

\begin{center}
\begin{tabular}{ l | c }
  Narrator  & Listen to the phrase for ``I'm fine'' or ``I feel fine'' \\
  Male Speaker & Me ho y\ga
  
\end{tabular}
\end{center}

In the domain of generating human language to both classifiy and describe signal inputs like images, recent research argues that this is not only possible, but likely already within reach \cite{reed2016learning} \cite{tran2016rich}. Automatic methods for image captioning, unsurprisingly, rely on large amounts of input images annotated with descriptions and typically a large and deep neural network to figure out the relationship between them. It is hardly reaching to suggest an analogous process for automatic speech pronunciation and intelligibility assessment. 

Moreover, in the domain of human language learning, it is often colloquially asserted that immersion training is the best way to practically learn a language. This is approach is soundly rooted in both theory and practice and is used in language training by organizations such as the U.S. Peace Corps and Foreign Service \cite{swain1998interaction} \cite{genesee1987learning} \cite{johnson1997immersion} \cite{howard2005second} \cite{leeds1990notes}. The practical academic discipline of field linguistics has also established a rigorous and broadly applicable methodology for discovering rules and structure in unknown languages \cite{seuren1966grammar} \cite{crowley2007field} \cite{lawler1998using}. 

There are certainly aspects of popular language learning platforms, immersion training approaches, field linguistics, and modern language technologies which can be constructively combined to make more effective oral language learning software. There is also a wealth of existing language resources from popular platforms which can be bootstrapped once a general framework is established. A resource which has only recently started to receive much attention for filling in the gaps, however, is crowdsourcing \cite{eskenazi2013crowdsourcing} \cite{parent2011speaking} \cite{callison2010creating}.

Using the wisdom of the crowd \cite{surowiecki2005wisdom}, many problems which today seem intractable can be approached incrementally. Automatically assessing pronunciation and intelligibility and generating feedback that assists people improve their oral skills in a given language in an empirically verifiable manner is well known to be a hard problem for computers which is relatively easy for human beings. It follows that crowdsourcing the work of pronunciation assessment is a good way to gather annotated data which can be used to iteratively improve machine models over time. We may not know the best automatic methods for generating feedback today, but it is certainly a worthwhile investment of time to begin gathering data for the future. Much like earning incremental interest and returns on an initial monetary investment, the passive gathering of data over time will reap dividends down the road.




